
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Bayesian Multiple Regression Example &#8212; bambi 0.1.1 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian Workflow Example (Police Officer’s Dilemma)" href="shooter_crossed_random_ANOVA.html" />
    <link rel="prev" title="Bayesian/Frequentist Tutorial" href="Bayesian_Frequentist_Tutorial.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }
</style>
<div class="section" id="Bayesian-Multiple-Regression-Example">
<h1>Bayesian Multiple Regression Example<a class="headerlink" href="#Bayesian-Multiple-Regression-Example" title="Permalink to this headline">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">bambi</span> <span class="k">as</span> <span class="nn">bmb</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
<div class="section" id="Load-and-examine-Eugene-Springfield-community-sample-data">
<h2>Load and examine Eugene-Springfield community sample data<a class="headerlink" href="#Load-and-examine-Eugene-Springfield-community-sample-data" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/ESCS.csv&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>drugs</th>
      <th>n</th>
      <th>e</th>
      <th>o</th>
      <th>a</th>
      <th>c</th>
      <th>hones</th>
      <th>emoti</th>
      <th>extra</th>
      <th>agree</th>
      <th>consc</th>
      <th>openn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
      <td>604.00</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.21</td>
      <td>80.04</td>
      <td>106.52</td>
      <td>113.87</td>
      <td>124.63</td>
      <td>124.23</td>
      <td>3.89</td>
      <td>3.18</td>
      <td>3.21</td>
      <td>3.13</td>
      <td>3.57</td>
      <td>3.41</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.65</td>
      <td>23.21</td>
      <td>19.88</td>
      <td>21.12</td>
      <td>16.67</td>
      <td>18.69</td>
      <td>0.45</td>
      <td>0.46</td>
      <td>0.53</td>
      <td>0.47</td>
      <td>0.44</td>
      <td>0.52</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.00</td>
      <td>23.00</td>
      <td>42.00</td>
      <td>51.00</td>
      <td>63.00</td>
      <td>44.00</td>
      <td>2.56</td>
      <td>1.47</td>
      <td>1.62</td>
      <td>1.59</td>
      <td>2.00</td>
      <td>1.28</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.71</td>
      <td>65.75</td>
      <td>93.00</td>
      <td>101.00</td>
      <td>115.00</td>
      <td>113.00</td>
      <td>3.59</td>
      <td>2.88</td>
      <td>2.84</td>
      <td>2.84</td>
      <td>3.31</td>
      <td>3.06</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2.14</td>
      <td>76.00</td>
      <td>107.00</td>
      <td>112.00</td>
      <td>126.00</td>
      <td>125.00</td>
      <td>3.88</td>
      <td>3.19</td>
      <td>3.22</td>
      <td>3.16</td>
      <td>3.56</td>
      <td>3.44</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.64</td>
      <td>93.00</td>
      <td>120.00</td>
      <td>129.00</td>
      <td>136.00</td>
      <td>136.00</td>
      <td>4.20</td>
      <td>3.47</td>
      <td>3.56</td>
      <td>3.44</td>
      <td>3.84</td>
      <td>3.75</td>
    </tr>
    <tr>
      <th>max</th>
      <td>4.29</td>
      <td>163.00</td>
      <td>158.00</td>
      <td>174.00</td>
      <td>171.00</td>
      <td>180.00</td>
      <td>4.94</td>
      <td>4.62</td>
      <td>4.75</td>
      <td>4.44</td>
      <td>4.75</td>
      <td>4.72</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>It’s always a good idea to start off with some basic plotting. Here’s
what our outcome variable ‘drugs’ (some index of self-reported illegal
drug use) looks like:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_5_0.png" src="../_images/notebooks_ESCS_multiple_regression_5_0.png" />
</div>
</div>
<p>The five predictor variables that we’ll use are sum-scores measuring
participants’ standings on the Big Five personality dimensions. The
dimensions are: - O = Openness to experience - C = Conscientiousness - E
= Extraversion - A = Agreeableness - N = Neuroticism</p>
<p>Here’s what our predictors look like:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="s1">&#39;c&#39;</span><span class="p">,</span><span class="s1">&#39;e&#39;</span><span class="p">,</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="s1">&#39;n&#39;</span><span class="p">]]);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_7_0.png" src="../_images/notebooks_ESCS_multiple_regression_7_0.png" />
</div>
</div>
</div>
<div class="section" id="Specify-model-and-examine-priors">
<h2>Specify model and examine priors<a class="headerlink" href="#Specify-model-and-examine-priors" title="Permalink to this headline">¶</a></h2>
<p>We’re going to fit a pretty straightforward multiple regression model
predicting drug use from all 5 personality dimension scores. It’s simple
to specify the model using a familiar formula interface. Here we tell
bambi to run two parallel Markov Chain Monte Carlo (MCMC) chains, each
one drawing 2000 samples from the joint posterior distribution of all
the parameters.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">fitted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="s1">&#39;drugs ~ o + c + e + a + n&#39;</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Sequential sampling (2 chains in 1 job)
INFO:pymc3:Sequential sampling (2 chains in 1 job)
NUTS: [drugs_sd, n, a, e, c, o, Intercept]
INFO:pymc3:NUTS: [drugs_sd, n, a, e, c, o, Intercept]
100%|██████████| 2000/2000 [00:29&lt;00:00, 68.95it/s]
100%|██████████| 2000/2000 [00:24&lt;00:00, 82.85it/s]
</pre></div></div>
</div>
<p>Great! But this is a Bayesian model, right? What about the priors?</p>
<p>If no priors are given explicitly by the user, then bambi chooses smart
default priors for all parameters of the model based on the implied
partial correlations between the outcome and the predictors. Here’s what
the default priors look like in this case – the plots below show 1000
draws from each prior distribution:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_12_0.png" src="../_images/notebooks_ESCS_multiple_regression_12_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Normal priors on the coefficients</span>
<span class="p">{</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">args</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">terms</span><span class="o">.</span><span class="n">values</span><span class="p">()}</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[7]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>{&#39;Intercept&#39;: {&#39;mu&#39;: array([2.21014664]), &#39;sd&#39;: array([7.49872428])},
 &#39;o&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.02706881])},
 &#39;c&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.03237049])},
 &#39;e&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.02957413])},
 &#39;a&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.03183624])},
 &#39;n&#39;: {&#39;mu&#39;: array([0]), &#39;sd&#39;: array([0.0264199])}}
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># Uniform prior on the residual standard deviation</span>
<span class="n">model</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">prior</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s1">&#39;sd&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">args</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[8]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>{&#39;lower&#39;: array(0), &#39;upper&#39;: array(0.64877877)}
</pre></div>
</div>
</div>
<p>Some more info about the default prior distributions can be found in
<a class="reference external" href="https://arxiv.org/abs/1702.01201">this technical paper</a>.</p>
<p>Notice the small SDs of the slope priors. This is due to the relative
scales of the outcome and the predictors: remember from the plots above
that the outcome, <code class="docutils literal notranslate"><span class="pre">drugs</span></code>, ranges from 1 to about 4, while the
predictors all range from about 20 to 180 or so. So a one-unit change in
any of the predictors – which is a trivial increase on the scale of the
predictors – is likely to lead to a very small absolute change in the
outcome. Believe it or not, these priors are actually quite wide on the
partial correlation scale!</p>
</div>
<div class="section" id="Examine-the-model-results">
<h2>Examine the model results<a class="headerlink" href="#Examine-the-model-results" title="Permalink to this headline">¶</a></h2>
<p>Let’s start with a pretty picture of the parameter estimates!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fitted</span><span class="o">.</span><span class="n">plot</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_19_0.png" src="../_images/notebooks_ESCS_multiple_regression_19_0.png" />
</div>
</div>
<p>The left panels show the marginal posterior distributions for all of the
model’s parameters, which summarize the most plausible values of the
regression coefficients, given the data we have now observed. These
posterior density plots show two overlaid distributions because we ran
two MCMC chains. The panels on the right are “trace plots” showing the
sampling paths of the two MCMC chains as they wander through the
parameter space.</p>
<p>A much more succinct (non-graphical) summary of the parameter estimates
can be found like so:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">fitted</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hpd0.95_lower</th>
      <th>hpd0.95_upper</th>
      <th>effective_n</th>
      <th>gelman_rubin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>3.306788</td>
      <td>0.350794</td>
      <td>2.669343</td>
      <td>4.043687</td>
      <td>1125.481830</td>
      <td>1.001945</td>
    </tr>
    <tr>
      <th>n</th>
      <td>-0.001545</td>
      <td>0.001171</td>
      <td>-0.003944</td>
      <td>0.000732</td>
      <td>1436.056288</td>
      <td>1.000048</td>
    </tr>
    <tr>
      <th>o</th>
      <td>0.006041</td>
      <td>0.001189</td>
      <td>0.003704</td>
      <td>0.008393</td>
      <td>1524.267732</td>
      <td>0.999558</td>
    </tr>
    <tr>
      <th>a</th>
      <td>-0.012419</td>
      <td>0.001449</td>
      <td>-0.015050</td>
      <td>-0.009481</td>
      <td>1408.252642</td>
      <td>1.003233</td>
    </tr>
    <tr>
      <th>e</th>
      <td>0.003371</td>
      <td>0.001320</td>
      <td>0.000809</td>
      <td>0.005919</td>
      <td>1694.707896</td>
      <td>1.000497</td>
    </tr>
    <tr>
      <th>c</th>
      <td>-0.003806</td>
      <td>0.001503</td>
      <td>-0.006978</td>
      <td>-0.001149</td>
      <td>1356.351417</td>
      <td>0.999537</td>
    </tr>
    <tr>
      <th>drugs_sd</th>
      <td>0.592951</td>
      <td>0.016867</td>
      <td>0.559014</td>
      <td>0.624859</td>
      <td>1438.667543</td>
      <td>0.999888</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>When there are multiple MCMC chains, the default summary output includes
some basic convergence diagnostic info (the effective MCMC sample sizes
and the Gelman-Rubin “R-hat” statistics), although in this case it’s
pretty clear from the trace plots above that the chains have converged
just fine.</p>
</div>
<div class="section" id="Summarize-effects-on-partial-correlation-scale">
<h2>Summarize effects on partial correlation scale<a class="headerlink" href="#Summarize-effects-on-partial-correlation-scale" title="Permalink to this headline">¶</a></h2>
<p>Let’s grab the samples and put them in a format where we can easily work
with them. We can do this really easily using the <code class="docutils literal notranslate"><span class="pre">to_df()</span></code> method of
the fitted <code class="docutils literal notranslate"><span class="pre">MCMCResults</span></code> object.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">fitted</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span>
<span class="n">samples</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Intercept</th>
      <th>n</th>
      <th>o</th>
      <th>a</th>
      <th>e</th>
      <th>c</th>
      <th>drugs_sd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.440476</td>
      <td>-0.001174</td>
      <td>0.007033</td>
      <td>-0.012882</td>
      <td>0.001259</td>
      <td>-0.003642</td>
      <td>0.562209</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.073972</td>
      <td>-0.001429</td>
      <td>0.006784</td>
      <td>-0.013615</td>
      <td>0.003824</td>
      <td>-0.001993</td>
      <td>0.612918</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.455090</td>
      <td>-0.001833</td>
      <td>0.006368</td>
      <td>-0.014293</td>
      <td>0.003591</td>
      <td>-0.003459</td>
      <td>0.609669</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.555880</td>
      <td>-0.001826</td>
      <td>0.005902</td>
      <td>-0.015004</td>
      <td>0.003906</td>
      <td>-0.003191</td>
      <td>0.564443</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.425497</td>
      <td>-0.001180</td>
      <td>0.005629</td>
      <td>-0.015116</td>
      <td>0.003624</td>
      <td>-0.002424</td>
      <td>0.588922</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>It turns out that we can convert each regresson coefficient into a
partial correlation by multiplying it by a constant that depends on (1)
the SD of the predictor, (2) the SD of the outcome, and (3) the degree
of multicollinearity with the set of other predictors. Two of these
statistics are actually already computed and stores in the fitted model
object, in a dictionary called <code class="docutils literal notranslate"><span class="pre">dm_statistics</span></code> (for design matrix
statistics), because they are used internally. The others we will
compute manually.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># the names of the predictors</span>
<span class="n">varnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">]</span>

<span class="c1"># compute the needed statistics</span>
<span class="n">r2_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">dm_statistics</span><span class="p">[</span><span class="s1">&#39;r2_x&#39;</span><span class="p">]</span>
<span class="n">sd_x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">dm_statistics</span><span class="p">[</span><span class="s1">&#39;sd_x&#39;</span><span class="p">]</span>
<span class="n">r2_y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">],</span>
                         <span class="n">exog</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">varnames</span> <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="n">x</span><span class="p">]]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">rsquared</span>
                  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">varnames</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">varnames</span><span class="p">)</span>
<span class="n">sd_y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># compute the products to multiply each slope with to produce the partial correlations</span>
<span class="n">slope_constant</span> <span class="o">=</span> <span class="n">sd_x</span><span class="p">[</span><span class="n">varnames</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r2_x</span><span class="p">[</span><span class="n">varnames</span><span class="p">])</span><span class="o">**.</span><span class="mi">5</span> \
    <span class="o">/</span> <span class="n">sd_y</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r2_y</span><span class="p">)</span><span class="o">**.</span><span class="mi">5</span>
<span class="n">slope_constant</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[12]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>o    32.392557
c    27.674284
e    30.305117
a    26.113299
n    34.130431
dtype: float64
</pre></div>
</div>
</div>
<p>Now we just multiply each sampled regression coefficient by its
corresponding <code class="docutils literal notranslate"><span class="pre">slope_constant</span></code> to transform it into a sampled partial
correlation coefficient.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pcorr_samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="n">varnames</span><span class="p">]</span> <span class="o">*</span> <span class="n">slope_constant</span>
<span class="n">pcorr_samples</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>o</th>
      <th>c</th>
      <th>e</th>
      <th>a</th>
      <th>n</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.227825</td>
      <td>-0.100794</td>
      <td>0.038152</td>
      <td>-0.336382</td>
      <td>-0.040077</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.219757</td>
      <td>-0.055156</td>
      <td>0.115887</td>
      <td>-0.355537</td>
      <td>-0.048786</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.206278</td>
      <td>-0.095714</td>
      <td>0.108840</td>
      <td>-0.373232</td>
      <td>-0.062565</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.191178</td>
      <td>-0.088297</td>
      <td>0.118359</td>
      <td>-0.391795</td>
      <td>-0.062335</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.182336</td>
      <td>-0.067090</td>
      <td>0.109831</td>
      <td>-0.394735</td>
      <td>-0.040289</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>And voilà! We now have a joint posterior distribution for the partial
correlation coefficients. Let’s plot the marginal posterior
distributions:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pcorr_samples</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">])</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[14]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.lines.Line2D at 0x7efc94126f28&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_32_1.png" src="../_images/notebooks_ESCS_multiple_regression_32_1.png" />
</div>
</div>
<p>The means of these distributions serve as good point estimates of the
partial correlations:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">pcorr_samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[15]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>a   -0.324302
c   -0.105328
n   -0.052734
e    0.102160
o    0.195684
dtype: float64
</pre></div>
</div>
</div>
<p>Naturally, these results are consistent with the OLS results. For
example, we can see that these estimated partial correlations are
roughly proportional to the t-statistics from the corresponding OLS
regression:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">endog</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;drugs&#39;</span><span class="p">],</span> <span class="n">exog</span><span class="o">=</span><span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span>
    <span class="n">data</span><span class="p">[</span><span class="n">varnames</span><span class="p">]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>drugs</td>      <th>  R-squared:         </th> <td>   0.176</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.169</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   25.54</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sat, 23 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>2.16e-23</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:00:37</td>     <th>  Log-Likelihood:    </th> <td> -536.75</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   604</td>      <th>  AIC:               </th> <td>   1086.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   598</td>      <th>  BIC:               </th> <td>   1112.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>
</tr>
<tr>
  <th>const</th> <td>    3.3037</td> <td>    0.360</td> <td>    9.188</td> <td> 0.000</td> <td>    2.598</td> <td>    4.010</td>
</tr>
<tr>
  <th>o</th>     <td>    0.0061</td> <td>    0.001</td> <td>    4.891</td> <td> 0.000</td> <td>    0.004</td> <td>    0.008</td>
</tr>
<tr>
  <th>c</th>     <td>   -0.0038</td> <td>    0.001</td> <td>   -2.590</td> <td> 0.010</td> <td>   -0.007</td> <td>   -0.001</td>
</tr>
<tr>
  <th>e</th>     <td>    0.0034</td> <td>    0.001</td> <td>    2.519</td> <td> 0.012</td> <td>    0.001</td> <td>    0.006</td>
</tr>
<tr>
  <th>a</th>     <td>   -0.0124</td> <td>    0.001</td> <td>   -8.391</td> <td> 0.000</td> <td>   -0.015</td> <td>   -0.010</td>
</tr>
<tr>
  <th>n</th>     <td>   -0.0015</td> <td>    0.001</td> <td>   -1.266</td> <td> 0.206</td> <td>   -0.004</td> <td>    0.001</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>10.273</td> <th>  Durbin-Watson:     </th> <td>   1.959</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.006</td> <th>  Jarque-Bera (JB):  </th> <td>   7.926</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.181</td> <th>  Prob(JB):          </th> <td>  0.0190</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.572</td> <th>  Cond. No.          </th> <td>3.72e+03</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.72e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.</div>
</div>
</div>
<div class="section" id="Relative-importance:-Which-predictors-have-the-strongest-effects-(defined-in-terms-of-partial-)?">
<h2>Relative importance: Which predictors have the strongest effects (defined in terms of partial <span class="math notranslate nohighlight">\(\eta^2\)</span>)?<a class="headerlink" href="#Relative-importance:-Which-predictors-have-the-strongest-effects-(defined-in-terms-of-partial-)?" title="Permalink to this headline">¶</a></h2>
<p>The partial <span class="math notranslate nohighlight">\(\eta^2\)</span> statistics for each predictor are just the
squares of the partial correlation coefficients, so it’s easy to get
posteriors on that scale too:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">pcorr_samples</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">2</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[17]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7efc94172080&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_ESCS_multiple_regression_39_1.png" src="../_images/notebooks_ESCS_multiple_regression_39_1.png" />
</div>
</div>
<p>With these posteriors we can ask: What is the probability that the
partial <span class="math notranslate nohighlight">\(\eta^2\)</span> for Openness (yellow) is greater than the partial
<span class="math notranslate nohighlight">\(\eta^2\)</span> for Conscientiousness (green)?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">pcorr_samples</span><span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">&gt;</span> <span class="n">pcorr_samples</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[18]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0.928
</pre></div>
</div>
</div>
<p>For each predictor, what is the probability that it has the largest
<span class="math notranslate nohighlight">\(\eta^2\)</span>?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">pcorr_samples</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pcorr_samples</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[19]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>a    0.992
o    0.008
dtype: float64
</pre></div>
</div>
</div>
<p>Agreeableness is clearly the strongest predictor of drug use among the
Big Five personality traits, but it’s still not a particularly strong
predictor in an absolute sense. Walter Mischel famously claimed that it
is rare to see correlations between personality measurse and relevant
behavioral outcomes exceed 0.3. In this case, the probability that the
agreeableness partial correlation exceeds 0.3 is:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pcorr_samples</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="o">.</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[20]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0.734
</pre></div>
</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">bambi</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="ANES_logistic_regression.html">Bayesian Logistic Regression Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Frequentist_Tutorial.html">Bayesian/Frequentist Tutorial</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Bayesian Multiple Regression Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Load-and-examine-Eugene-Springfield-community-sample-data">Load and examine Eugene-Springfield community sample data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Specify-model-and-examine-priors">Specify model and examine priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Examine-the-model-results">Examine the model results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Summarize-effects-on-partial-correlation-scale">Summarize effects on partial correlation scale</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Relative-importance:-Which-predictors-have-the-strongest-effects-(defined-in-terms-of-partial-)?">Relative importance: Which predictors have the strongest effects (defined in terms of partial <span class="math notranslate nohighlight">\(\eta^2\)</span>)?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="shooter_crossed_random_ANOVA.html">Bayesian Workflow Example (Police Officer’s Dilemma)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Strack_RRR_re_analysis.html">Bayesian Workflow Example (Strack RRR Analysis Replication)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">API Reference</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="../examples.html">Examples</a><ul>
      <li>Previous: <a href="Bayesian_Frequentist_Tutorial.html" title="previous chapter">Bayesian/Frequentist Tutorial</a></li>
      <li>Next: <a href="shooter_crossed_random_ANOVA.html" title="next chapter">Bayesian Workflow Example (Police Officer’s Dilemma)</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, The developers of Bambi.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.9</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.11</a>
      
      |
      <a href="../_sources/notebooks/ESCS_multiple_regression.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>